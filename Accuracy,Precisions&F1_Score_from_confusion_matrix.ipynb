{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/o15wNNDhbcgmBoGkt9S+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KDManikandan/25DaysInMachineLearning/blob/master/Accuracy%2CPrecisions%26F1_Score_from_confusion_matrix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Confusion Matrix**\n",
        "\n",
        "To understand the confusion matrix let us consider a two-class classification problem with the two outcomes being “Positive” and “Negative”. Given a data point to predict, the model’s outcome will be any one of these two.\n",
        "\n",
        "If we plot the predicted values against the ground truth (actual) values, we get a matrix with the following representative elements:\n",
        "\n",
        "True Positives (TP): These are the data points whose actual outcomes were positive and the algorithm correctly identified it as positive.\n",
        "\n",
        "True Negatives (TN): These are the data points whose actual outcomes were negative and the algorithm correctly identified it as negative.\n",
        "\n",
        "False Positives (FP): These are the data points whose actual outcomes were negative but the algorithm incorrectly identified it as positive.\n",
        "\n",
        "False Negatives (FN): These are the data points whose actual outcomes were positive but the algorithm incorrectly identified it as negative.\n",
        "\n",
        "As you can guess, the goal of evaluating a model using the confusion matrix is to maximize the values of TP and TN and minimize the values of FP and FN.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jokq8yjmGwE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Precision**\n",
        "\n",
        "Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while Recall (also known as sensitivity) is the fraction of the total amount of relevant instances that were actually retrieved. Both precision and recall are therefore based on an understanding and measure of relevance.\n",
        "\n",
        "In simple terms, precision means what percentage of the positive predictions made were actually correct.\n",
        "\n",
        "Precision=TP/TP+FP"
      ],
      "metadata": {
        "id": "JM8GJbk-HwLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**F1  Score**\n",
        "\n",
        "In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test’s accuracy. It is calculated from the precision and recall of the test, where the precision is the number of correctly identified positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive"
      ],
      "metadata": {
        "id": "uYjVbvhhIGY_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWEcvTstGsoJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}